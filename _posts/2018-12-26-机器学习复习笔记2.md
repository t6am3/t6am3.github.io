### 15.K均值算法
* 使用聚类标注向量，对应的类别标签置1，其他置0
* 准则函数的设计是为了评估所有样本属于该类的合理程度
* 准则函数用来将样本初始化分类，使他们分为靠类样本中心最近的类
* 当标签被初始化之后，通过求准则函数的极值，优化类样本中心
* 如此循环，迭代样本所属类别与每个类别的样本中心。这样可以使准则函数越来越小，也即Expectation & Maximization

### 16.EM算法EM算法是一种期望最大化的迭代算法，用于含有隐变量或潜变量概率模型参数的极大似然估计或极大后验概率估计，其具体步骤为：
* 选择参数初值
* E步骤：估计潜变量后验分布，计算期望
* M步骤：最大化期望，更新参数
* 检验停止条件，若不满足则继续迭代

### 17.混合高斯模型  [相关博客](https://blog.csdn.net/lin_limin/article/details/81048411)这篇文章写的很好，推荐看  
> 混合高斯模型实际上就是多个高斯模型的加权混合，用来拟合各种各样的分布

### 18.数据降维
> 在原始的高维空间中，包含冗余信息和噪声信息，会在实际应用中引入误差，影响准确率；而降维可以提取数据内部的本质结构，减少冗余信息和噪声信息造成的误差，提高应用中的精度。降维是利用某种映射将原高维度空间的数据投影到低维度的空间

### 19.主成分分析（PCA）> 使用较少的数据维度保留住较多的原数据特性
* 最大方差思想：经过数学推导，发现投影向量应该为样本方差的特征向量，当投影向量为最大特征值对应的特征向量时，方差取到极大值，称此投影向量为第一主成分
* 最小均方误差思想：使原数据与降维后的数据（在原空间中的重建）的误差最小，实际上就是使共享部分的误差最小，即让降维后损失的信息最少。经过推导，要使这样的误差最小，其对应着最小的若干个特征值，则对应在投影的时候应该使用最大的若干个特征值。
* 在使用PCA处理高维数据时，样本维数会超高，求方差矩阵

### 20.深度学习-神经网络
* ​感知器（单个神经元可以实现两类问题的现行分类），多个感知器则可以实现多类别问题的线性问题。理论证明，三层感知器可以实现任意的逻辑运算，在激活函数为Sigmoid函数的情况下，可以逼近任何非线性多元函数
* 反向传播算法（BP，Back Propagation）
* 从后向前反向逐层传播输出层的误差，以间接计算隐层的误差
	* 前馈（正向过程）：从输入层经隐层逐层正向计算各单元的输出
	* 学习（反向过程）：由输出误差逐层反向计算隐层各单元的误差，并用此误差修正前层的权值
* 梯度消失与梯度爆炸：因为传输了过多层，在反向传播的时候，由于权值与激活函数的导数都是小于1的有界数，连乘之后会使越前面层的梯度非常小。同理，若是放大权值，则会导致梯度爆炸
* 随着隐含层增加，反向传播传递给较低层的信息会减少。实际上，由于信息向前反馈，不同层间的梯度开始消失，对网络中权重的影响也会变小
* 深度学习可能会导致过拟合。因为模型非常复杂，会对训练数据有非常好的识别效果，而对真实样本的识别效果非常差
* 预训练：在没有建立输入数据到输出标记的映射的条件下，将网络里的权重值初始化至一个合适的位置
* 微调：加入全连接层，整个网络变成一个多层的感知机，使用反向传播的方法进行训练


### 21.深度学习-卷积神经网络（CNN）Convolutional Neural Network
* 使用卷积
* 使用线性整流激活函数(Rectified Linear Units, ReLU)
	* 这样的激活函数避免了梯度爆炸和梯度消失问题
	* 简化了计算过程（导数很好求）
	* 训练稀疏网络（？）
	* 池化操作
    * 减少参数和计算量，防止过拟合
	* 使模型对尺度、平移、旋转变化有一定的不变形

### 22.深度学习-如何有效训练深度神经网络
六种算法：
* ​SGD（Stochastic Gradient Descent）随机梯度下降/BGD批梯度下降/MBGD小批量梯度下降
    * 每个训练轮次使用单个样本的梯度进行参数更新
    * 会引起梯度震荡
    * 相似样本，冗余计算
	* 学习率难以选择
	* 对于非凸函数，容易陷入到局部极小值和鞍点
* Adagrad（Adaptive gradient）自适应梯度更新（可以使用线性衰减或者指数衰减的学习率）
	* 对稀疏参数进行大幅更新，对频繁参数进行小幅更新
	* 适合处理稀疏数据
	* Adagrad引起学习率衰减
* RMSprop（Root Mean Square Prop）[相关博客](https://www.cnblogs.com/cloud-ken/p/7728427.html)：另外一种形式运用动量来减少震荡
* 动量（Momentum）学习率：与上一次的梯度下降方向作加权平均和，这样可以有效地消去偏离方向，更加重合向极值方向。这次梯度方向和上次梯度方向的合成
* NAG（Nesterov Momentum update）使用（假如使用动量方向获得的位置的梯度）与上次梯度方向的合成
* Adadelta：过去一段时间的梯度将影响学习率

> 其他技巧：
* ​正则化-Dropout：随机关闭一些神经元
	* 降低模型参数
	* 强制使网络有冗余表示
	* 每次dropout都得到一个新模型
	* 最终结果是多个模型的融合
* ​批归一（Batch normalization）
	* 对激活后的输出归一化
	* 可以选择较大的学习率
	* 可以减少或不适用dropout
	* 缓解梯度消失/爆炸问题

### 23.集成学习
> 将多个学习器进行整合，可获得比单一学习器显著优越的泛化性能，这对弱分类器尤为明显
* ​串行化方法：个体学习器间存在强依赖关系时使用：Boosting
	* 一族可以将弱学习器提升为强学习器的算法
		* 个体学习器间存在强依赖关系
		* 串行生成
		* 每次调整训练数据的样本分布
	* 先从初始数据集中训练出一个基学习器，再根据它对训练样本分布进行调整，使先前做错的样本在后续收到更多的关注，重复这一过程得到许多的基学习器，最终将这些基学习器加权结合
* 并行化方法：个体学习器间不存在强依赖关系：Bagging、随机森林
	* Bagging
		* 自助采样法
			* 给定包含若干个样本的数据集，对其进行采样产生数据集
			* 每次随机从其中原样本集中挑选一个样本，将其拷贝到产生的数据集里，执行原来数据集大小次数的拷贝后对拷贝后的数据集进行考察
			* 用拷贝生成的数据集作为训练集，用它们的差集做为测试集
		* 使用自助采样法构造若干个采样集，使用它们分别训练处基学习器，然后对它们进行结合（分类任务通常使用投票法。回归问题使用简单平均法）
	* 随机森林是Bagging的一种扩展变体
		* 使用决策树作为基学习器
		* 训练过程引入随机属性决策
		* 对基决策树的每个节点，先从属性集合中选取一个子集，使用这个子集来寻找最优属性进行划分，
	* 基学习器多样性通过样本扰动和属性扰动来实现
	* 性能非常强大，被誉为代表集成学习技术水平的方法
	* 算法简单，容易实现，计算开销小
* 学习器的结合策略
	* 统计方面：减小误选假设空间导致泛化性能不佳的几率
	* 技术方面：降低陷入坏局部点极小点影响泛化性能的风险
	* 表示方面：扩大假设空间学习对于真实空间更好的近似

### 24.概率图模型
> 每个点都应该乘
* ​结点是随机变量或者集合
* 边是随机变量之间的关系
* 有向图为贝叶斯网，无向图为马尔科夫网[马尔可夫链](https://blog.csdn.net/bitcarmanlee/article/details/82819860)
	* （Markov Random Fields）马尔科夫随机场是典型的马尔科夫网，是著名的无向图模型
	* 对于马尔科夫场来说，结点表示变量（集），边表示依赖关系
	* 势函数（potential functions）称为因子，是定义在变量子集上的非负实函数，主要用于定义概率分布函数
	* 马尔科夫随机场中极大团是指极大全联通子图
	* 多个变量之间的连续分布可基于团分解为多个因子的乘积，每个因子只与一个团相关


