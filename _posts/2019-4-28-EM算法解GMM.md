# EM算法解混合高斯模型(GMM)

#### <center><font face='楷体'>计算机学院   SY1806106   刘 幼峰</font></center>

## References

> 真正需要推公式的时候还是查阅了许多资料的，其中主要包括但不限于以下资料：
>
> * CS229 lecture notes: Mixtures of Gaussians and the EM algorithm
> * CS229 lecture notes: The EM algorithm
> * [详解EM算法与混合高斯模型](<https://blog.csdn.net/lin_limin/article/details/81048411>)
> * [矩阵求导术](<https://zhuanlan.zhihu.com/p/24709748>)
> * Matrixcookbook

## 关于混合高斯模型

> 混合高斯模型实际上是一种隐马尔科夫模型。
>
> 如果已知现在的数据分别满足确定类别数的不同的高斯分布，而我们采样的时候以不同的先验概率从这些高斯分布中采样，则我们获得的一个混合的样本则属于一个混合高斯模型。
>
> 对于这个混合的样本，我们并不知道其中每个样本点具体服从哪个特定参数的高斯分布，但是确实知道这其中有多少个高斯模型的混合，这时候每个样本点的所属分类则为隐变量，我们无法对于这个模型进行直接的优化求解  。
>
> 在隐马尔科夫模型中很重要的一点就是你现在要估计的分布被一个隐含变量决定，而你无法显式的获得这个隐变量，现在我们面临的是一个无监督问题，这时候我们需要用到EM算法来解决这类问题。

## 关于EM算法

> 当我们求解一般的高斯模型参数时，比如我们获得一组样本点，已知其满足高斯分布，我们想要求得其具体参数。由于函数形式已知，我们可以使用极大似然估计法，令导数为0来求得使该组样本点出现概率最大的一组参数。
>
> 对于混合高斯模型问题，如果我们使用相同的办法去求解其参数，会遇到无法求解的问题。
>
> 实际上，  假设我们的训练集为 $\{x^{(1)}， x^{(2)}， x^{(3)}， ...， x^{(n)}\}$， 
>
> 假设现在有$k$类高斯模型， 且这些高斯模型的混合系数为$z$， 我们希望联合的概率密度为$p(x^{(i)}， z^{(i)})=p(x^{(i)}|z^{(i)})p(z^{(i)})$， 其中$z^{(i)}$满足一个多项分布$Multinomial(\phi)$
>
> 则我们想要求的最大似然联合概率的公式如下:
> $$
> l(\phi, \mu, \Sigma) = \sum_{i=1}^{m}logp(x^{(i)};\phi, \mu, \Sigma)\\
> =\sum_{i=1}^{m}log\sum_{z^{(i)}=1}^kp(x^{(i)}|z^{(i)};\mu,\Sigma)p(z^{(i)};\phi)
> $$
> 假设我们直接置偏导为0， 比如说对$\mu_{j}$求导， 其中$\mu_{j}$为第$j$个高斯模型的均值， 可以看到上述公式中存在$log$中求和的情况， 这样即使置导数为0， 我们得到的方程也是个超越方程， 无法解得答案。
>
> 在此问题中变量$z^{(i)}$就是隐藏着样本点所属高斯分布类别信息的隐变量，我们发现在上述公式前一个等号要是直接能够求解是非常方便的，只可惜$p(x^{(i)};\phi, \mu, \Sigma)$无法直接写出来，因为实际上$\phi$是隐变量$z$的参数，如果我们知道隐变量的值的话，这个问题用极大似然法来解就很方便了，因为$z^{(i)}$已知，上述公式的求和符号可以被去掉。
>
> * 如果我们认为$z^{(i)}$是一个参数为$\phi$的隐变量，我们每次考虑的是一个由全概率公式的到的求和式
> * 如果我们知道$z^{(i)}$是一个固定的值，则整个式子就被简化成为一个纯粹的极大似然估计求解问题，因为这时我们认为每个样本点是明确属于一类的

> 这两种思路就是我们在机器学习问题中一直在纠缠的两种思路，对于一个待定量，我们有时候认为它是一个待解的常量参数，有时候我们认为其是一个满足一定分布的变量
>
> 使用EM的主要思路就是首先估计样本点的所属类别，然后根据估计求解极大似然估计参数，然后再次对样本点进行分类估计，再次根据估计求解极大似然估计参数，是一个迭代的过程。

> 在Andrew Ng的笔记中使用$Jensen's\quad inequality$来证明了为何使用EM算法获得的结果是收敛的，使用这个不等式可以为联合概率构建一个下界，而我们每次的优化都是收紧下界的过程，这样EM算法获得的结果是一定会收敛的。由于篇幅限制这里并不详细对证明过程进行说明。在Ng的笔记中直接给出了三个参数$(\phi,\mu, \Sigma)$的迭代解公式，实际上的过程应该是：
>
> * 先进行E-step：在给定参数下，如果已知一个样本点，估计其属于任何一类的概率（对隐变量的估计），也即是样本点后验分布，这样我们就可以得到一个最紧的下界的表达式，这时候样本点后验分布已经变成已知的了。
> * 再进行M-step：我们优化这个最紧下界，求其参数的极大似然估计，并运用到下一次后验概率分的估计中。
>
> 实际求解过程EMstep与我脑海里面的EM算法相当不同，在我的脑海里的EM算法应该是这样的：
>
> * 先进行E-step：根据概率估计隐变量（估计每一个样本点的具体分类）
> * 再进行M-step：由于每一个样本点的具体分类已知，则我们只要求解一个具体纯粹的极大似然问题，可以迭代出新的参数$(\phi,\mu, \Sigma)$
>
> 我的想法和具体的实现方法差距很大，我会在稍后的部分进行具体的讨论。
>
> 鉴于这个部分是对算法理念的描述，在下个部分我会仔细谈一谈我的具体实现过程
>
> 在Ng的笔记中直接给出了三个参数的解$(\phi,\mu, \Sigma)$，并且给出了其中$\phi$与$\mu$的推导过程，我在这里将给出$\Sigma$的解的推导过程，这其中我查找了许多关于标量、向量以及矩阵的求导资料，十分感谢知乎老哥的无私分享，让我对矩阵求导算是不再那么迷茫。
>
> 首先，给出三个参数的解的公式：
> $$
> \phi_j = \frac 1 m \sum_{i=1}^m\omega_j^{(i)}
> $$
>
> $$
> \mu_j = \frac {\sum_{i=1}^m\omega_j^{(i)}x^{(i)}} {\sum_{i=1}^m\omega_j^{(i)}}
> $$
>
> $$
> \Sigma_j = \frac {\sum_{i=1}^m\omega_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T} {\sum_{i=1}^m\omega_j^{(i)}}
> $$
>
> $\Sigma_j$的求导过程：
>
> 由于下界的显式公式如下：
> $$
> \sum_{i=1}^m\sum_{j=1}^k\omega_j^{(i)}log\frac {\frac{1}{(2\pi)^{n/2}|\Sigma_j|^{1/2}}exp(-\frac12(x^{i}-\mu_j)^T\Sigma_j^{-1}(x^{i}-\mu_j))·\phi_j}{\omega_j^{(i)}}
> $$
> 故对于$\Sigma_l$的导数为：
> $$
> \nabla_{\Sigma_l}\sum_{i=1}^m\sum_{j=1}^k\omega_j^{(i)}log\frac {\frac{1}{(2\pi)^{n/2}|\Sigma_j|^{1/2}}exp(-\frac12(x^{i}-\mu_j)^T\Sigma_j^{-1}(x^{i}-\mu_j))·\phi_j}{\omega_j^{(i)}}\\
> =-\frac12\sum_{i=1}^m\omega_l^{(i)}\nabla_{\Sigma_l}(log|\Sigma_l| + (x^{i}-\mu_l)^T\Sigma_l^{-1}(x^{i}-\mu_l))\\
> $$
> 这里涉及到对$\Sigma_l$行列式值以及其逆的求导，查找资料并通过微分法则推导出
> $$
> \frac {d|X|}{dX}=|X|X^{-1}
> $$
> 又
> $$
> \frac{d(a^TYb)}{dY}=ab^T
> $$

通过协方差阵的对称性质

> $$
> 若Y=X^{-1}, \frac{d(a^TYb)}{dX}=-X^{-1}\frac{d(a^TYb)}{dX}X^{-1}
> $$
>
> 代入上式，得到
> $$
> -\frac12\sum_{i=1}^m\omega_l^{(i)}(\frac{|\Sigma_l|}{|\Sigma_l|}\Sigma_l^{-1}-\Sigma_l^{-1}(x^{(i)}-\mu_l)(x^{(i)}-\mu_l)^T\Sigma_l^{-1})
> $$
> 令这个式子为0，解得
> $$
> \Sigma_l = \frac {\sum_{i=1}^m\omega_l^{(i)}(x^{(i)}-\mu_l)(x^{(i)}-\mu_l)^T} {\sum_{i=1}^m\omega_l^{(i)}}
> $$

# 实现过程

我使用numpy中自带的生成高维正态分布样本的方法生成样本数据， 并将其以已知比例混合，得到一个待估计的混合样本，通过EM算法收敛来求得其具体参数。我的实验使用两组参数不同的二维高斯样本

* 初始化样本参数$\mu,\Sigma, \pi$，其中$\mu$为两个样本的均值，$\Sigma$为协方差阵，$\pi$为混合比例

* 重复以下过程，直至收敛：

  * E-step：求得样本属于具体哪个类别的后验分布：由于在我的试验中，只分了两类高斯分布，则任意一点属于两类的概率分别为
    $$
    e_j^{(i)} = \pi_jN(x^{(i)}|\mu_j,\Sigma_j)
    $$
    这样后验分布为
    $$
    \omega_j^{(i)} = \frac{e_j^{(i)}}{\sum e}
    $$

  * M-step：通过公式计算下一次迭代的三个参数

* 收敛的条件为参数不再发生任何变化

# 过程可视化

我固定调整两个样本的均值相差$(5, 5)$，协方差阵为正定对称阵，混合比例$1:1$，将参数初始化为两个样本均值与协方差阵的平均值，分布系数初始化为$1:9$

每当分布系数参数之差变化了0.1，我将打印出一张变化的图片，在完全收敛后，得到最后一张图片。

其中在Initail Pic中的参数legend为真实参数，其他图片中为实时参数。

绘制闭合曲线的依据是使用matplotlib中的contour函数，绘制出对应样本集若使用某组参数的概率密度的等高线，在我的实验中取0.01，即所有概率密度为0.01的点的等高线。

![](D:\pyWork\NLP03\Thu_Apr_25_20_27_43_2019\0th iterations later.png)

![](D:\pyWork\NLP03\Thu_Apr_25_20_27_43_2019\77th iterations later.png)

![](D:\pyWork\NLP03\Thu_Apr_25_20_27_43_2019\79th iterations later.png)

![](D:\pyWork\NLP03\Thu_Apr_25_20_27_43_2019\81th iterations later.png)

![](D:\pyWork\NLP03\Thu_Apr_25_20_27_43_2019\84th iterations later.png)

![](D:\pyWork\NLP03\Thu_Apr_25_20_27_43_2019\87th iterations later.png)

![](D:\pyWork\NLP03\Thu_Apr_25_20_27_43_2019\91th iterations later.png)

![](D:\pyWork\NLP03\Thu_Apr_25_20_27_43_2019\168th iterations later.png)

考虑到生成样本时的噪音，最后收敛的样本参数与真实参数没有区别。

## 关于收敛

我做了比较多次的实验，快的不到100次就收敛了，慢的要好几百次才收敛，甚至有一些情况会在反复的两组参数间一直跳，不过这种情况极少极少，我只出现过一次。

另外，虽然我没有绘制出收敛速度随次数变化的图，但是在图的标题中就可以看出的很明显的一点就是在前面很长一段时间基本不怎么变化（0~77），当过了一定阈值后疯狂收敛（77至91），然后再以比较缓慢的速度收敛到最优点。如果不绘图的话，其实解得一个模型的速度相当之快。

# 总结与扩展

* 我在这次作业中详细的学习和实现了一遍EM算法，对于其数学本质有了一遍了解，另外，也学习到了更多的数学代数及求导知识。

* 我尝试了一些神经网络的方法来解GMM模型。

  * 我使用两个样本的混合样本点作为输入，整个模型的参数作为输出，尝试了以下的网络结构
    * 多层全连接
    * 卷积\池化 加全连接

  * 由于理解和推导EM算法的数学层花费了太多时间，并没有尝试更多的网络结构，但是总的来说，神经网络运用在GMM上得到的效果并不如人意，一般的acc只有百分之几，最好的也只有45。我认为需要更加复杂的网络结构才可以支持GMM的解

* 我并未实验我的想法的EM算法解，以后有时间我会进行尝试。关于我的想法与Ng的具体推导过程的区别：
  * Ng的算法是以优化紧下界为目标的。为了得到紧下界，必须求隐变量的后验分布，这里使用上一次迭代的结果来计算，这是在数学上的需要。这样得到的结果是有着数学上严格的收敛性的。
  * 我的想法是先对样本的分布通过原先参数所得概率进行估计，在对联合概率密度进行优化。我这种想法来源于上个学期中学习机器学习聚类中的kNN聚类，实际上它也是EM算法的一种。而且并没有对收敛性进行严格的证明。

* 在我理解里，EM算法应该是一种更为泛化的方法，但是在Ng的EM算法里，它更像是一种数学trick，为了达成目的反推出每个步骤应要做的事情。这一点我很疑惑，需要老师来解答。

  