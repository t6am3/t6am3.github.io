---
layout: page
title:  "二进制手工搭建k8s集群"
---
### [注]本文修改于[原博客地址](https://www.cnblogs.com/sammyliu/p/8182078.html)注意的是本文使用K8sv1.10.4版本，在一些地方和原博客的部署已经不同。比如在原博客配置etcd的配置使用的.conf文件，而这里使用的是.yml文件<br>
# 二进制手工搭建K8s集群
### 作者：刘幼峰<br>
### 最后更新时间：2018/12/24
### 完成状态：刚刚部署完毕，还未验证，除了systemctl status etcd.service还未正常外其他均正常

## 部署K8s集群机器信息

|机器名称|ip地址|节点身份|etcd|flanneld|需要安装的K8s组件|docker|
|---+---+---+---+---+---+---|
|super09|192.168.7.78|master|Y|Y|kubctl<br>kube-apiserver<br>kube-controller-manager<br>kube-scheduler|Y|
|super08|192.168.7.77|node1|Y|Y|kube-proxy<br>kubelet|Y|
|===+===+===+===+===+===+===|

# 一、etcd
---
## 什么是etcd？[官方网址](https://coreos.com/etcd/)上的概述的是，A distributed, reliable key-value store for the most critical data of a distributed system. etcd被用在K8s集群中是用来保存集群所有的网络配置和对象的状态信息，也是一个分布式存储系统。K8s系统中的flanneld组件和K8s自己的服务都需要用到etcd来协同和存储配置[相关博客](https://www.cnblogs.com/softidea/p/6517959.html)。具体的用途如下：
* 网络插件flannel、对于其它网络插件也需要用到etcd存储网络的配置信息
* kubernetes本身，包括各种对象的状态和元信息配置

## 原理：Etcd使用的是raft一致性算法来实现的，是一款分布式的一致性KV存储，主要用于共享配置和服务发现。

## 安装etcd及配置
使用以下命令安装etcd<br>
`ETCD_VERSION=${ETCD_VERSION:-"3.3.4"}`<br>`
ETCD="etcd-v${ETCD_VERSION}-linux-amd64"`<br>`
curl -L https://github.com/coreos/etcd/releases/download/v${ETCD_VERSION}/${ETCD}.tar.gz -o etcd.tar.gz`<br>`
tar xzf etcd.tar.gz -C /tmp`<br>`
mv /tmp/etcd-v${ETCD_VERSION}-linux-amd64 /opt/bin/`<br>

注意，所有的机器最好使用相同版本的etcd，因为super08的etcd版本为3.3.4，所以这里也使用3.3.4<br>
可以使用scp命令从已经有相应配置的节点机器上拷贝二进制文件以及配置文件<br>
`/opt/bin/(etcd & etcdctl), /opt/config/etcd.yml, /lib/systemd/system/etcd.service`<br>
配置文件需要修改：修改etcd.yml文件

具体的文件配置<br>
`/lib/systemd/system/etcd.service`<br>
<pre><code>
[Unit]
Description=Etcd Server
Documentation=https://github.com/coreos/etcd
After=network.target
After=network-online.target 
Wants=network-online.target

[Service]
User=root
Type=simple
WorkingDirectory=/opt/bin/
ExecStart=/opt/bin/etcd --config-file=/opt/config/etcd.yml
Restart=on-failure
RestartSec=10s
LimitNOFILE=40000

[Install]
WantedBy=multi-user.target
</code></pre>

(master)/opt/config/etcd.yml
<pre><code>
name: master
data-dir: /var/lib/etcd  
listen-client-urls: http://192.168.7.78:2379,http://127.0.0.1:2379
advertise-client-urls: http://192.168.7.78:2379,http://127.0.0.1:2379  
listen-peer-urls: http://192.168.7.78:2380  
initial-advertise-peer-urls: http://192.168.7.78:2380 
initial-cluster: master=http://192.168.7.78:2380,node01=http://192.168.7.77:2380
initial-cluster-token: etcd-cluster-token  
initial-cluster-state: new
</code></pre>


## 重启etcd服务
关闭etcd服务（如果原先启动了的话）<br>
`systemctl stop etcd.service`

将原来部署过的节点的`/var/lib/etcd/`中的member文件改成member.bak或者直接删掉,否则无法正常的启动

启动etcd服务<br>
`systemctl daemon-reload`<br>
`systemctl enable etcd`<br>
`systemctl restart etcd.service`

## 验证etcd启动状态
使用`etcdctl member list 以及etcdctl cluster-health`测试etcd的运行状态<br>
需要注意的是，你使用`systemctl status etcd`去查看会发现master上面还有红的，而node1上甚至全是红的不匹配。<br>不用理会<br>
你如果更改node1上的yml把new改成exsiting也没有什么用，如果你信了master的报错把etcd.service中的simple改成notify也没有什么用，亲测，只要上面两条测试命令没问题就ok<br>
出现这种问题的原因是在master和node1的配置文件etcd.yml中一开始设定的集群状态(initial-cluster-state)都是new，这样才可以正常启动，但是启动之后需要统一集群ID，这时候按照原教程应该修改为existing，但是尝试过也没有用，目前仍不清楚具体原因，但是实际上使用etcdctl查看集群的运行状态是ok的

# 二、flanneld
## 什么是flanneld？flanneld是CoreOS提供用于解决Docker集群跨主机通讯的覆盖网络工具。它的主要思路是：预先留出一个网段，每个主机使用其中一部分，然后每个容器被分配不同的ip；让所有的容器认为大家在同一个直连的网络，底层通过UDP/VxLAN等进行报文的封装和转发。我的理解是：实现docker容器的跨主机通讯，想了解更多关于flanneld可以点[这里](https://www.hi-linux.com/posts/30481.html)
## 安装flanneld及配置
使用以下命令安装：<br>
`curl -L https://github.com/coreos/flannel/releases/download/v0.8.0/flannel-v0.8.0-linux-amd64.tar.gz flannel.tar.gz`<br>`
tar xzf flannel.tar.gz -C /tmp`<br>`
mv /tmp/flanneld /opt/bin/`<br>
同样可以使用scp命令拷贝`/lib/systemd/system/flanneld.service`<br>,记得修改endpoints以及iface两项
其中iface改成本机IP

配置文件：
(master)/lib/systemd/system/flanneld.service
<pre><code>
[Unit]
Description=Flanneld
Documentation=https://github.com/coreos/flannel
After=network.target
Before=docker.service
[Service]
User=root
ExecStart=/opt/bin/flanneld \
#-etcd-endpoints="http://192.168.7.78:2379,http://192.168.7.77:2379" \
#-iface=192.168.7.78 \
#-ip-masq
Restart=on-failure
Type=notify
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
</code></pre>

## 重启flanneld
（可能原先的节点上还运行着原来配置的）<br>
`systemctl daemon-reload`<br>
`systemctl enable flanneld`<br>
`systemctl restart flanneld`

## 验证flanneld启动成功
在master机器上运行<br>
`etcdctl --endpoints="http://192.168.7.78:2379,http://192.168.7.77:2379" mk /coreos.com/network/config \ '{"Network":"10.1.0.0/16", "Backend": {"Type": "vxlan"}}'`

使用<br>`/opt/bin/etcdctl --endpoints="http://192.168.7.77:2379" ls /coreos.com/network/subnets`<br>
返回<br>
`/coreos.com/network/subnets/10.1.9.0-24`<br>
`/coreos.com/network/subnets/10.1.66.0-24`<br>
在所有机器上使用`ifconfig flannel.1`命令
查看是否对应<br>
可以使用`service flanneld status`查看状态

# 三、docker
## 关于docker：Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。[docker中文社区](http://www.docker.org.cn/) | [菜鸟教程](http://www.runoob.com/docker/docker-tutorial.html)
## 安装docker
super集群上已经装有`docker-ce17.03`, 不用再装
## 配置docker
配置docker
使用scp在新的节点上从老节点上把/usr/bin/mk-docker-opts.sh拉过来
在每个节点上运行<br>
`mk-docker-opts.sh -i`<br>
`source /run/flannel/subnet.env`<br>
`ifconfig docker0`<br>
`ifconfig docker0 ${FLANNEL_SUBNET}`<br>
`ifconfig docker0`<br>

将新节点`/lib/systemd/system/docker.service`文件删掉，拉老节点的过来<br>
具体的文件内容为：
<pre><code>
[Unit]
After=network.target docker.socket firewalld.service
Requires=docker.socket

[Service]
Type=notify
#exists and systemd currently does not support the cgroup feature set required
#for containers run by docker
EnvironmentFile=-/etc/default/docker
ExecStart=/usr/bin/dockerd -H fd:// $DOCKER_OPTS
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=1048576
#Having non-zero Limit*s causes performance problems due to accounting overhead
#in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity
#Uncomment TasksMax if your systemd version supports it.
#Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0
#set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes
#kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
</code></pre>
## 放开iptables规则[为什么要放开规则](https://blog.csdn.net/zhao4471437/article/details/53216666)
`iptables -P OUTPUT ACCEPT`<br>
`iptables -P FORWARD ACCEPT`<br>
`iptables-save`<br>

## 重启docker服务
`systemctl daemon-reload`<br>
`systemctl enable docker`<br>
`systemctl restart docker`<br>

## 验证docker
使用命令`docker run -it ubuntu bash`可以生成一个Ubuntu容器，互相可以ping通（虽然没有ping命令。。）


# 四、创建证书与配置
## 生成master的证书
在master上，创建`master_ssl.cnf`文件
使用命令<br>
`openssl genrsa -out ca.key 2048`<br>
`openssl req -x509 -new -nodes -key ca.key -subj "/CN=company.com" -days 10000 -out ca.crt`<br>
`openssl genrsa -out server.key 2048`<br>
`openssl req -new -key server.key -subj "/CN=master" -config master_ssl.cnf -out server.csr`<br>
`openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 10000 -extensions v3_req -extfile master_ssl.cnf -out server.crt`<br>
`openssl genrsa -out client.key 2048`<br>
`openssl req -new -key client.key -subj "/CN=node" -out client.csr`<br>
`openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 10000`<br>
因为super集群上的节点可能又没有装oepnssl。。用`apt-get install openssl`

将生成的所有文件移到`/root/key/`下，把里面的ca.crt和ca.key文件拷贝到节点的用户桌面/kub/下

## 生成node的证书
在所有非master上用户桌面/kub/下运行以下命令：（可能又要安openssl，有些节点需要更换apt源）
(这里有个超级大坑：有些机子安装openssl使用`apt-get install`会安装一个很奇怪的版本，用不了，所以要使用`apt-get install openssl=1.0.2g-1ubuntu4.14`指定版本号安装)<br>
`CLINET_IP=192.168.7.77`<br>
`/usr/bin/openssl genrsa -out client.key 2048`<br>
`/usr/bin/openssl req -new -key client.key -subj "/CN=${CLINET_IP}" -out client.csr`<br>
`/usr/bin/openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 10000`<br>
这时候该文件夹里有6个文件，将 client 和 ca 的 .crt 和 .key 拷贝至 `/root/key` 文件夹

## 配置node上K8s配置文件
创建`/etc/kubernetes/kubeconfig`文件，内容如下<br>
<pre><code>
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /root/key/ca.crt
    server: https://192.168.7.78:6443
  name: ivic
contexts:
- context:
     cluster: ivic
    user: ivic
  name: ivic
current-context: ivic
kind: Config
preferences: {}
users:
- name: ivic
  user:
    client-certificate: /root/key/client.crt
    client-key: /root/key/client.key
</code></pre>
记得缩进

# 五、maste节点布置
## 安装K8s
安装1.10.4版本<br>
`curl -L https://dl.k8s.io/v1.10.4/kubernetes-server-linux-amd64.tar.gz kuber.tar.gz`<br>
`tar xzf kuber.tar.gz -C /tmp3`<br>
`mv /tmp3/kubernetes/server/bin/* /opt/bin`<br>
（以上命令好像不能正常工作，所以可以拷自己下载的Kubernetes二进制码进/opt/bin/, 就直接使用它给的地址下载就行了）

## master上K8s配置文件
创建`/root/.kube/config`
<pre><code>
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /root/key/ca.crt
  name: ivic
contexts:
- context:
    cluster: ivic
    user: ivic
  name: ivic
current-context: ivic
kind: Config
preferences: {}
users:
- name: ivic
  user:
    client-certificate: /root/key/client.crt
    client-key: /root/key/client.key 
</code></pre>
注意缩进一定要严格遵守，否则无法成功运行

## 配置[apiserver服务](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/)
创建`/lib/systemd/system/kube-apiserver.service`
<pre><code>
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
User=root
ExecStart=/opt/bin/kube-apiserver \
--secure-port=6443 \
--etcd-servers=http://192.168.7.78:2379,http://192.168.7.77:2379 \
--logtostderr=false \
--log-dir=/var/log/kubernetes \
--allow-privileged=false \
--service-cluster-ip-range=192.1.0.0/16 \
--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,SecurityContextDeny,ResourceQuota \
--service-node-port-range=30000-32767 \
--advertise-address=192.168.7.78 \
--client-ca-file=/root/key/ca.crt \
--tls-cert-file=/root/key/server.crt \
--tls-private-key-file=/root/key/server.key
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>

## 配置[controller-manager服务](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/)
创建`/lib/systemd/system/kube-controller-manager.service`
<pre><code>
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
User=root
ExecStart=/opt/bin/kube-controller-manager \
--master=https://192.168.7.78:6443 \
--root-ca-file=/root/key/ca.crt \
--service-account-private-key-file=/root/key/server.key \
--kubeconfig=/root/.kube/config \
--logtostderr=false \
--log-dir=/var/log/kubernetes
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>

## 配置[scheduler服务](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/)
创建`/lib/systemd/system/kube-scheduler.service`
<pre><code>
[Unit]<br>
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
User=root
ExecStart=/opt/bin/kube-scheduler \
--logtostderr=false \
--log-dir=/var/log/kubernetes \
--master=https://192.168.7.78:6443 \
--kubeconfig=/root/.kube/config
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>

## 启动/重启所有的服务
`systemctl daemon-reload`<br>`
systemctl enable kube-apiserver`<br>`
systemctl enable kube-controller-manager`<br>`
systemctl enable kube-scheduler`<br>`
systemctl enable flanneld`<br>`
systemctl restart kube-apiserver`<br>`
systemctl restart kube-controller-manager`<br>`
systemctl restart kube-scheduler`<br>

可以使用以下命令验证组件运行情况：<br>
`systemctl status kube-apiserver`<br>`
systemctl status kube-controller-manager`<br>`
systemctl status kube-scheduler `<br>


# 六、非master节点配置
## 配置[kubelet服务](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)
创建`/lib/systemd/system/kubelet.service`
<pre><code>
[Unit]
Description=Kubernetes Kubelet
After=docker.service
Requires=docker.service

[Service]
ExecStart=/opt/bin/kubelet \
--hostname-override=192.168.7.77 \
--pod-infra-container-image="docker.io/kubernetes/pause" \
--cluster-domain=cluster.local \
--log-dir=/var/log/kubernetes \
--cluster-dns=192.1.0.100 \
--kubeconfig=/etc/kubernetes/kubeconfig \
--logtostderr=false
Restart=on-failure
KillMode=process
[Install]
WantedBy=multi-user.target
[Unit]
Description=Kubernetes Proxy
After=network.target
</code></pre>

## 配置[kube-proxy服务](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/)
创建`/lib/systemd/system/kube-proxy.service`
<pre><code>
[Unit]
Description=Kubernetes Proxy
After=network.target

[Service]
ExecStart=/opt/bin/kube-proxy \
--hostname-override=192.168.7.77 \
--master=https://192.168.7.78:6443 \
--log-dir=/var/log/kubernetes \
--kubeconfig=/etc/kubernetes/kubeconfig \
--logtostderr=false
Restart=on-failure

[Install]
WantedBy=multi-user.target
</code></pre>

## 验证K8s集群启动状况
可以使用以下命令验证组件运行情况<br>
`systemctl status kubelet`<br>`
systemctl status kube-proxy`

# 七、集群验证
使用`kubectl cluster-info以及kubectl get nodes`命令来验证
至此，集群搭建完毕

