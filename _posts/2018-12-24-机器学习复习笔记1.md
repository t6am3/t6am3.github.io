[矩阵求导](https://blog.csdn.net/daaikuaichuan/article/details/80620518)  
> ## 机器学习主要问题的分类

|离散或者连续|监督学习|非监督学习|
|---+---+---|
|离散|分类器|聚类|
|连续|回归|降维|

___
### 1.回归模型中可以使用多种多样的基函数，使线性地模型非线性化
* 多项式基函数
* 高斯基函数
* Sigmoid基函数


### 2.过拟合与欠拟合
> 训练模型的时候会遇到欠拟合（underfiting）与过拟合（overfiting），当多项式的回归模型的阶数一定时，增加样本数目会导致过拟合，这个时候多项式系数可能会非常的不健康，这时候可以使用正则化（Regularization）来对较大值的系数进行惩罚，（即加上系数的范式，当系数大的时候，判别函数会被惩罚）

### 3.贝叶斯分类：从先验概率出发，利用贝叶斯公式来获得后验概率。
> 贝叶斯公式实际上是对先验概率的修正。在似然函数中提供了对先验概率的延伸，再由证据因子来对先验概率进行条件下的修正。

### 4.如何决策
* 基于最小错误率的决策：谁的后验概率更高就选谁
* 基于最小风险的决策：作出每种决策都有可能犯错，而犯错就会造成一定的损失。这时候应该对决策的风险来进行评估，种决策的风险更小就选谁



### 5.使用贝叶斯理论来进行决策的几种情况
* 如果类条件概率密度参数未知？（即似然函数）：只需要类条件概率密度的参数表达式形式是确定的，就可以利用样本来估计类条件概率密度的未知参数，这里可以使用极大似然
* 如果类条件概率密度的形式都无法确定，就使用非参数方法来估计（频率估计）：这需要大量样本



### 6.线性判别函数
> 构造线性分类器（超平面），通过样本对这个分类器进行训练，训练的准则由准则函数来确定，通过求准则函数的极值就可以设计好线性分类器。

### 7.广义线性判别函数
>使用多项式的构建方法来提高线性判别函数的划分范围，这样做可以利用线性函数的简单性来解决复杂问题，但是会导致维数大大增加，即“维数灾难”。注：即使是广义的线性判别函数，也不适用于非凸与多连通区域的划分。

### 8.如何具体设计线性分类器？
* 线性判别函数的系数矩阵如何得来？因为最好的结果一般都出现在准则函数的极值点上，所以将分类器的设计转化为了求准则函数的极值（变量设为系数）的问题
* 步骤1：准备样本集（需要标签）
* 步骤2：确定准则函数，准则函数需要为样本集以及判别函数系数的函数
* 步骤3：优化求解准则函数极值，得到最优的判别函数系数



### 9.Fisher准则
> 把高维的样本投影到一维上，使得样本在这个方向上的投影分开的最好。
如何寻找最好的投影方向？  
* 首先，对于高维空间里时：
    * 对于各类的样本，求其均值向量
    * 求每个样本类内离散度矩阵和总类内离散度矩阵，其中求总类内离散度矩阵可以加权
    * 求样本类见离散度矩阵
* 投影之后，在一维空间
    * 也具有各类样本均值
    * 也具有样本类内离散度和总类内离散度
* 我们的目的是希望投影后各类样本分尽量分开，即（均值之差）越大越好；在这同时，我们还希望各类样本内部尽量密集，即类内的离散度越小越好
* 所以Fisher准则的准则函数为均值之差平方除以各类类内离散度平方和
* 由于准则函数中包含我们的投影方向，我们通过对该准则函数来求最值来获得最好的投影方向  
[Fisher准则函数寻找最优投影方向的推导](https://blog.csdn.net/dan1900/article/details/19478033)
* 最后还需要确定分界阈值，在博客里也有详细的说明



### 10.感知机（perception）
> 感知机是一种自学习的判别函数生成方法。对于任意给定的判别函数初始值，会随着样本分类训练过程对其进行逐步的修正（感知机需要对两种样本进行规范化）  
那么，如何去寻找解向量呢？
* 对于一组规范化的增广样本向量，寻找解向量使他们的内积大于0
* 对于线性可分的问题，构造的准则函数即为小于0的样本内积和，错分的时候，准则函数就会变大，只有当找到找到了真正的解向量时准则函数才会为0
* 使用梯度下降法来求解解向量
* 感知机的结果并不唯一，且在线性不可分情况下不收敛



11.最小二乘准则
> 最小化误差的平方和来寻找最佳函数匹配
* 最小二乘准则根据不同的准则均值选择可以逼近其他准则的解
* 由于直接通过方程法求解计算量可能过大，一般还是采用梯度下降法来求
* 该算法对异常值非常敏感



12.SVM | [SVM相关的博客](https://blog.csdn.net/lisi1129/article/details/70209945?locationNum=8&fps=1)
> SVM与线性判别不同的地方在于是去求最优分类面（使空隙尽可能的大）
* 核心思想：使离分离平面最近的点离分离平面的距离尽可能的远
* 做法：假设支持向量离平面的函数间隔为1，求这时候得到最大的几何间隔时的参数值，这时候问题从二次规划问题转化为了带约束的极值问题，通过对偶形式去求解，最后问题转化为求拉格朗日系数的函数的最值问题，其中对于每个拉格朗日系数都对应着一个支持向量
* 使用松弛变量来在准确性和泛化性中作trade-off，允许部分的异常点，把硬间隔转化为软间隔，异常点将会施加惩罚
* 因为很多情况下都是线性不可分，需要把样本投影到高维空间使其线性可分，但是这样的做法会导致很庞大的计算量。我们引入核函数的概念，使用简单的运算来实现复杂的运算 | [核函数详解地址](https://blog.csdn.net/kateyabc/article/details/79980880)
* 无论是否引入松弛，最后的问题都已经被转化为了求拉格朗日系数的问题，我们可以使用坐标轮换法，首先确定一组初值的拉格朗日系数，寻找其中偏离最远的两个来进行优化，直到求出最优的所有的拉格朗日系数的解，即得到确定的SVM分类面
* SVM的多分类实现使用1 VS others 来实现



### 13.决策树
> 对于每个特征来进行判断，分为不同的类，在所有的特征判断完毕将样本完全划分，采用自顶向下的递归方法，以信息熵为度量构造一棵熵值下降最快的树，到叶子结点处的熵值为零，此时每个叶结点中的实例都属于同一类（属性和特征这里是同一个意思）  
如何建立决策树
* Step1：选取一个属性作为决策树的根节点，然后就这个属性所有的取值创建树的分支
* Step2：用这棵树来对训练数据集进行分类
    * 如果一个叶节点的所有实例都属于同一类，则以该类为标记标识此叶节点
    * 如果所有的叶节点都有类标记，则算法终止
* Step3：选取一个从该节点到根路径中没有出现过得属性作为标记标识该节点，然后就这个属性所有的取值继续创建树的分支，然后重复Step2

> 主要的算法
* ID3（Iterative Dichotomiser 3）算法
    * 以信息熵作为度量，每次优先选取信息增益最大的属性，即能使熵值最小的属性
	* 熵是随机变量不确定的度量

	* 信息熵/经验熵/条件熵……
	* 信息增益：特征对训练数据的信息增益定义为集合的经验熵与特征给定条件下集合的经验条件熵之差
	* 什么是特征给定条件下集合的经验条件熵？一个特征可能有多个取值，在数据集中对该特征每个同样取值的子集的信息熵乘以该子集的权重求和即位特征给定条件下集合的经验条件熵。由于这个值为负，所以越小信息增益会越大，注意求信息增益是拿原来的减现在的
	* 最后的终止条件为：待选特征集合为空，或者分支已经全部属于同一类。如果最后特征集合为空，分支仍不属于同一类，则选取最大比重的作为分支的标记
* 决策树是一种推理算法，学习的是人类的决策过程。ID3算法具有一定的局限性，具体如下：

	* 信息增益会偏好取值多的属性
	* 该方法会受噪声或者小样本的影响，易出现过拟合
	* 无法处理连续值的属性
	* 无法处理属性值不完整的训练数据
	* 无法处理不同代价的属性
* 使用信息增益率，使用某种特征时的信息增益除以某种特征的固有值。当取值数越多的时候，特征的固有值也会越大，使用这种算法可以缓解信息增益准则对可取值数目较多的属性的偏好。C4.5算法采用增益率替代了信息增益
* 属性筛选度量的其它标准：基尼指数，基尼指数反映的是从数据集中随机抽取两个样本，其类别不一致的概率；基尼指数越小，数据集的纯度越高。CART算法就采用基尼指数替代了信息增益，每次选取基尼指数最小的属性来分支
* 剪枝处理：针对过拟合问题。基本策略如下

	* 预剪枝策略（pre-pruning）：在生成决策树的过程中，对于每个节点在划分前进行估计，若划分不能带来决策树泛化性能提升，则停止划分，并将该节点设为叶节点。降低了过拟合的风险，减少了训练时间开销，但是有可能停止了后续分支可能带来的泛化性能提升，有可能导致欠拟合。
	* 后剪枝策略（post-pruning）：生成决策树后，自底向上对非叶节点进行考察，若将该节点对应的子树替换成叶节点能带来泛化性能提升，则替换。测试过了所有分支的后剪枝策略比预剪枝泛化性能更好，降低了欠拟合的风险，但是计算成本较高。
	* 泛化性能指的是，测试集被正确划分的正确率





